#PDR-APPä¸­æ–‡.py
import streamlit as st
import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
import io
import base64
from datetime import datetime
import re
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="PDRé£é™©é¢„æµ‹ç³»ç»Ÿ",
    page_icon="ğŸ‘ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# === æ ¸å¿ƒå®šä¹‰ï¼š20ä¸ªç‰¹å¾ ===
# ä¸­è‹±æ–‡ç‰¹å¾æ˜ å°„å­—å…¸ - ä½¿ç”¨ä¸‹åˆ’çº¿æ ¼å¼
FEATURE_MAPPING = {
    # åŸºæœ¬ä¿¡æ¯
    'æ€§åˆ«': 'Sex',
    'å¹´é¾„': 'Age',
    'ç³–å°¿ç—…ç—…ç¨‹': 'Course',
    'BMI': 'BMI',
    'è…°è‡€æ¯”': 'WHR',
    'æ”¶ç¼©å‹': 'SBP',
    'èˆ’å¼ å‹': 'DBP',
    'é«˜è¡€å‹ç—…ç¨‹': 'duration_of_HT',  # ä½¿ç”¨ä¸‹åˆ’çº¿æ ¼å¼
    # å®éªŒå®¤æŒ‡æ ‡
    'è¡€å°¿ç´ æ°®': 'BUN',
    'è¡€æ¸…è‚Œé…': 'Scr',
    'å°¿é…¸': 'UA',
    'æ€»è›‹ç™½': 'TP',
    'ç™½è›‹ç™½': 'ALB',
    'æ€»èƒ†çº¢ç´ ': 'TBIL',
    'ç›´æ¥èƒ†çº¢ç´ ': 'DBIL',
    'è°·ä¸™è½¬æ°¨é…¶': 'ALT',
    'è°·è‰è½¬æ°¨é…¶': 'AST',
    'ç©ºè…¹è¡€ç³–': 'FBG',
    'ç³–åŒ–è¡€çº¢è›‹ç™½': 'HbA1c',
    'å°¿ç™½è›‹ç™½æ’æ³„ç‡': 'UAER'
}

# åå‘æ˜ å°„ï¼ˆè‹±æ–‡åˆ°ä¸­æ–‡ï¼‰
REVERSE_FEATURE_MAPPING = {v: k for k, v in FEATURE_MAPPING.items()}

# ç‰¹å¾åˆ†ç»„
FEATURE_GROUPS = {
    'basic': {
        'name': 'åŸºæœ¬ä¿¡æ¯',
        'features': ['æ€§åˆ«', 'å¹´é¾„', 'ç³–å°¿ç—…ç—…ç¨‹', 'BMI', 'è…°è‡€æ¯”', 'æ”¶ç¼©å‹', 'èˆ’å¼ å‹',
                     'é«˜è¡€å‹ç—…ç¨‹']
    },
    'advanced': {
        'name': 'å®éªŒå®¤æŒ‡æ ‡',
        'features': ['è¡€å°¿ç´ æ°®', 'è¡€æ¸…è‚Œé…', 'å°¿é…¸', 'æ€»è›‹ç™½', 'ç™½è›‹ç™½', 'æ€»èƒ†çº¢ç´ ', 'ç›´æ¥èƒ†çº¢ç´ ',
                     'è°·ä¸™è½¬æ°¨é…¶', 'è°·è‰è½¬æ°¨é…¶', 'ç©ºè…¹è¡€ç³–', 'ç³–åŒ–è¡€çº¢è›‹ç™½', 'å°¿ç™½è›‹ç™½æ’æ³„ç‡']
    }
}

# æ¨¡å‹éœ€è¦çš„ç‰¹å¾ï¼ˆ20ä¸ªç‰¹å¾ï¼‰- ä½¿ç”¨ä¸‹åˆ’çº¿æ ¼å¼
MODEL_FEATURES_EN = [
    'Sex', 'Age', 'Course', 'BMI', 'WHR', 'SBP', 'DBP', 'BUN', 'Scr', 'UA',
    'TP', 'ALB', 'TBIL', 'DBIL', 'ALT', 'AST', 'FBG', 'HbA1c', 'UAER', 'duration_of_HT'
]

# æ•°å€¼å‹ç‰¹å¾ - ä½¿ç”¨ä¸‹åˆ’çº¿æ ¼å¼
NUMERIC_FEATURES = [
    'Age', 'Course', 'BMI', 'WHR', 'SBP', 'DBP', 'BUN', 'Scr', 'UA', 'TP',
    'ALB', 'TBIL', 'DBIL', 'ALT', 'AST', 'FBG', 'HbA1c', 'UAER', 'duration_of_HT'
]
CATEGORICAL_FEATURES = ['Sex']

# åˆ†ç±»ç‰¹å¾çš„æ˜ å°„å…³ç³»
CATEGORY_MAPPINGS = {
    'Sex': {
        'male': 1, 'ç”·': 1, 'ç”·æ€§': 1, '1': 1, '1.0': 1,
        'female': 2, 'å¥³': 2, 'å¥³æ€§': 2, '2': 2, '2.0': 2,
        'default': 2  # é»˜è®¤å¥³æ€§
    }
}

# === å…¨å±€å˜é‡ç”¨äºç¼“å­˜æ¨¡å‹ ===
if 'debug_mode' not in st.session_state:
    st.session_state.debug_mode = False


# === è¾“å…¥å­—æ®µåˆ›å»ºå‡½æ•° ===
def create_input_field_chinese(feature_name):
    """æ ¹æ®ä¸­æ–‡ç‰¹å¾ååˆ›å»ºè¾“å…¥å­—æ®µ"""
    if feature_name == 'æ€§åˆ«':
        options = [("å¥³æ€§", 2), ("ç”·æ€§", 1)]
        selected = st.selectbox("æ€§åˆ«", options=options, format_func=lambda x: x[0])
        return selected[1]
    elif feature_name == 'å¹´é¾„':
        return st.number_input("å¹´é¾„ï¼ˆå²ï¼‰", min_value=0, max_value=120, value=50, step=1)
    elif feature_name == 'ç³–å°¿ç—…ç—…ç¨‹':
        return st.number_input("ç³–å°¿ç—…ç—…ç¨‹ï¼ˆå¹´ï¼‰", min_value=0.0, max_value=50.0, value=5.0, step=0.5)
    elif feature_name == 'BMI':
        return st.number_input("BMIï¼ˆä½“é‡æŒ‡æ•°ï¼‰", min_value=10.0, max_value=50.0, value=24.0, step=0.1)
    elif feature_name == 'è…°è‡€æ¯”':
        return st.number_input("è…°è‡€æ¯”ï¼ˆWHRï¼‰", min_value=0.5, max_value=1.5, value=0.9, step=0.01)
    elif feature_name == 'é«˜è¡€å‹ç—…ç¨‹':
        return st.number_input("é«˜è¡€å‹ç—…ç¨‹ï¼ˆå¹´ï¼‰", min_value=0.0, max_value=50.0, value=0.0, step=0.5)
    elif feature_name == 'æ”¶ç¼©å‹':
        return st.number_input("æ”¶ç¼©å‹ï¼ˆmmHgï¼‰", min_value=60.0, max_value=250.0, value=120.0, step=1.0)
    elif feature_name == 'èˆ’å¼ å‹':
        return st.number_input("èˆ’å¼ å‹ï¼ˆmmHgï¼‰", min_value=20.0, max_value=250.0, value=80.0, step=1.0)
    elif feature_name == 'è¡€å°¿ç´ æ°®':
        return st.number_input("è¡€å°¿ç´ æ°®ï¼ˆBUN, mmol/Lï¼‰", min_value=1.0, max_value=30.0, value=5.0, step=0.1)
    elif feature_name == 'è¡€æ¸…è‚Œé…':
        return st.number_input("è¡€æ¸…è‚Œé…ï¼ˆScr, Î¼mol/Lï¼‰", min_value=20.0, max_value=500.0, value=70.0, step=1.0)
    elif feature_name == 'å°¿é…¸':
        return st.number_input("å°¿é…¸ï¼ˆUA, Î¼mol/Lï¼‰", min_value=100.0, max_value=800.0, value=300.0, step=1.0)
    elif feature_name == 'æ€»è›‹ç™½':
        return st.number_input("æ€»è›‹ç™½ï¼ˆTP, g/Lï¼‰", min_value=40.0, max_value=100.0, value=70.0, step=0.1)
    elif feature_name == 'ç™½è›‹ç™½':
        return st.number_input("ç™½è›‹ç™½ï¼ˆALB, g/Lï¼‰", min_value=20.0, max_value=60.0, value=45.0, step=0.1)
    elif feature_name == 'æ€»èƒ†çº¢ç´ ':
        return st.number_input("æ€»èƒ†çº¢ç´ ï¼ˆTBIL, Î¼mol/Lï¼‰", min_value=1.0, max_value=100.0, value=12.0, step=0.1)
    elif feature_name == 'ç›´æ¥èƒ†çº¢ç´ ':
        return st.number_input("ç›´æ¥èƒ†çº¢ç´ ï¼ˆDBIL, Î¼mol/Lï¼‰", min_value=0.0, max_value=50.0, value=4.0, step=0.1)
    elif feature_name == 'è°·ä¸™è½¬æ°¨é…¶':
        return st.number_input("è°·ä¸™è½¬æ°¨é…¶ï¼ˆALT, U/Lï¼‰", min_value=5.0, max_value=200.0, value=25.0, step=1.0)
    elif feature_name == 'è°·è‰è½¬æ°¨é…¶':
        return st.number_input("è°·è‰è½¬æ°¨é…¶ï¼ˆAST, U/Lï¼‰", min_value=5.0, max_value=200.0, value=26.0, step=1.0)
    elif feature_name == 'ç©ºè…¹è¡€ç³–':
        return st.number_input("ç©ºè…¹è¡€ç³–ï¼ˆFBG, mmol/Lï¼‰", min_value=3.0, max_value=30.0, value=6.5, step=0.1)
    elif feature_name == 'ç³–åŒ–è¡€çº¢è›‹ç™½':
        return st.number_input("ç³–åŒ–è¡€çº¢è›‹ç™½ï¼ˆHbA1c, %ï¼‰", min_value=4.0, max_value=15.0, value=6.5, step=0.1)
    elif feature_name == 'å°¿ç™½è›‹ç™½æ’æ³„ç‡':
        return st.number_input("å°¿ç™½è›‹ç™½æ’æ³„ç‡ï¼ˆUAER, Î¼g/minï¼‰", min_value=0.0, max_value=500.0, value=20.0, step=1.0)
    else:
        return st.number_input(f"{feature_name}", min_value=0.0, max_value=1000.0, value=50.0, step=1.0)


# === åŠ å¼ºç‰ˆæ•°æ®æ¸…æ´—å‡½æ•° ===
def clean_numeric_dataframe(df):
    """æ¸…æ´—æ•°å€¼æ•°æ®ï¼Œå¤„ç†ä¸­æ–‡é€—å·/ç©ºæ ¼/ç©ºå­—ç¬¦ä¸²"""
    df_clean = df.copy()

    for col in df_clean.columns:
        # å…ˆå°†æ•´ä¸ªåˆ—è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œç„¶åè¿›è¡Œæ¸…æ´—
        df_clean[col] = df_clean[col].astype(str).apply(lambda x: x.strip() if isinstance(x, str) else x)

        def clean_value(x):
            # å¦‚æœæ˜¯NaNæˆ–None
            if pd.isna(x):
                return np.nan

            # å¦‚æœæ˜¯å­—ç¬¦ä¸²
            if isinstance(x, str):
                x_str = x.strip()

                # å¦‚æœæ¸…æ´—åæ˜¯ç©ºå­—ç¬¦ä¸²
                if x_str == '':
                    return np.nan

                # å¤„ç†å¸¸è§çš„ä¸­æ–‡ç¬¦å·
                x_str = x_str.replace('ï¼Œ', '.').replace(',', '.').replace(' ', '')

                # å°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°
                try:
                    return float(x_str)
                except:
                    # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œå°è¯•æå–æ•°å­—éƒ¨åˆ†
                    try:
                        numbers = re.findall(r'-?\d+\.?\d*', x_str)
                        if numbers:
                            return float(numbers[0])
                        else:
                            return np.nan
                    except:
                        return np.nan
            # å¦‚æœä¸æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›
            return x

        df_clean[col] = df_clean[col].apply(clean_value)

    return df_clean


# === ä¸‹è½½é“¾æ¥å‡½æ•° ===
def get_table_download_link(df, filename="é¢„æµ‹ç»“æœ.xlsx", format="excel"):
    """ç”Ÿæˆä¸‹è½½é“¾æ¥"""
    df_display = df.copy()
    # åˆ—åè½¬æ¢ä¸ºä¸­æ–‡
    column_mapping = {}
    for col in df_display.columns:
        if col in REVERSE_FEATURE_MAPPING:
            column_mapping[col] = REVERSE_FEATURE_MAPPING[col]
        elif col in ['PDRé¢„æµ‹æ¦‚ç‡', 'PDRé¢„æµ‹ç±»åˆ«', 'PDRé£é™©ç­‰çº§', 'é¢„æµ‹æ—¶é—´']:
            column_mapping[col] = col

    if column_mapping:
        df_display = df_display.rename(columns=column_mapping)

    if format == "excel":
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df_display.to_excel(writer, index=False, sheet_name='é¢„æµ‹ç»“æœ')
        excel_data = output.getvalue()
        b64 = base64.b64encode(excel_data).decode()
        href = f'<a href="data:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;base64,{b64}" download="{filename}">ğŸ“¥ ä¸‹è½½é¢„æµ‹ç»“æœ (Excel)</a>'
    else:
        csv = df_display.to_csv(index=False, encoding='utf-8-sig')
        b64 = base64.b64encode(csv.encode()).decode()
        href = f'<a href="data:file/csv;base64,{b64}" download="{filename.replace(".xlsx", ".csv")}">ğŸ“¥ ä¸‹è½½é¢„æµ‹ç»“æœ (CSV)</a>'
    return href


# === æ¨¡å‹åŠ è½½å‡½æ•° ===
@st.cache_resource
def load_model_and_preprocessors():
    """åŠ è½½æ¨¡å‹å’Œé¢„å¤„ç†å·¥å…·"""
    try:
        # åŠ è½½æ¨¡å‹
        model = joblib.load('final_results/lightgbm_pdr_model.pkl')

        # æ£€æŸ¥æ¨¡å‹çš„ç‰¹å¾åç§°
        if hasattr(model, 'feature_name_'):
            model_features = model.feature_name_
            st.info(f"ğŸ“‹ æ¨¡å‹ä½¿ç”¨çš„ç‰¹å¾: {model_features}")

        # åŠ è½½scaler
        scaler = joblib.load('final_results/scaler.pkl')

        # æ£€æŸ¥scalerçš„ç‰¹å¾
        if hasattr(scaler, 'feature_names_in_'):
            st.info(f"ğŸ“‹ æ ‡å‡†åŒ–å™¨è®­ç»ƒæ—¶çš„ç‰¹å¾: {scaler.feature_names_in_}")
            st.info(f"ğŸ“‹ æ ‡å‡†åŒ–å™¨è®­ç»ƒæ—¶çš„ç‰¹å¾æ•°é‡: {len(scaler.feature_names_in_)}")

        # åŠ è½½imputers
        median_imputer = joblib.load('final_results/median_imputer.pkl')
        mode_imputer = joblib.load('final_results/mode_imputer.pkl')

        # å°è¯•åŠ è½½selected_features
        try:
            selected_features = pd.read_csv('final_results/selected_features.csv').iloc[:, 0].tolist()
            # ç¡®ä¿ç‰¹å¾é¡ºåºä¸MODEL_FEATURES_ENä¸€è‡´
            selected_features = [f for f in MODEL_FEATURES_EN if f in selected_features]
        except:
            selected_features = MODEL_FEATURES_EN.copy()

        # ç‰¹å¾ä¿¡æ¯
        feature_info = {
            'numeric_features': NUMERIC_FEATURES,
            'categorical_features': CATEGORICAL_FEATURES,
            'selected_features': selected_features,
            'median_imputer': median_imputer,
            'mode_imputer': mode_imputer,
            'scaler': scaler,
            'model_features': MODEL_FEATURES_EN
        }

        return model, scaler, feature_info, selected_features

    except Exception as e:
        st.error(f"âŒ åŠ è½½å¤±è´¥: {e}")
        import traceback
        st.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return None, None, None, None


# === åˆ†ç±»ç‰¹å¾æ ‡å‡†åŒ–å‡½æ•° ===
def standardize_categorical_feature(series, feature_name):
    """æ ‡å‡†åŒ–åˆ†ç±»ç‰¹å¾"""
    if feature_name not in CATEGORY_MAPPINGS:
        return series

    mapping = CATEGORY_MAPPINGS[feature_name]

    def map_value(x):
        if pd.isna(x):
            return mapping['default']

        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¤„ç†
        x_str = str(x).strip().lower()

        # æ£€æŸ¥æ˜ å°„
        for key, value in mapping.items():
            if key == 'default':
                continue
            if x_str == key.lower():
                return value

        # å°è¯•æ•°å€¼è½¬æ¢
        try:
            val = float(x_str)
            if val in [0, 1, 2]:
                return int(val)
        except:
            pass

        # è¿”å›é»˜è®¤å€¼
        return mapping['default']

    return series.apply(map_value)


# === é¢„å¤„ç†å‡½æ•° ===
def preprocess_batch_data(batch_df, feature_info):
    """æ‰¹é‡æ•°æ®é¢„å¤„ç†"""
    # 1. æ•°æ®æ¸…æ´—
    batch_df = clean_numeric_dataframe(batch_df)

    # 2. åˆ—åè½¬æ¢ï¼ˆä¸­æ–‡â†’è‹±æ–‡ï¼‰
    column_mapping = {}
    for col in batch_df.columns:
        col_clean = str(col).strip()
        if col_clean in FEATURE_MAPPING:
            column_mapping[col] = FEATURE_MAPPING[col_clean]
        elif col_clean in MODEL_FEATURES_EN:
            column_mapping[col] = col_clean

    if column_mapping:
        batch_df = batch_df.rename(columns=column_mapping)

    # 3. å¼ºåˆ¶å¯¹é½æ¨¡å‹éœ€è¦çš„ç‰¹å¾
    batch_df_aligned = pd.DataFrame(index=batch_df.index)

    for feature in MODEL_FEATURES_EN:
        if feature in batch_df.columns:
            batch_df_aligned[feature] = batch_df[feature]
        else:
            # è®¾ç½®é»˜è®¤å€¼
            if feature == 'Sex':
                batch_df_aligned[feature] = 2  # é»˜è®¤å¥³æ€§
            elif feature == 'Age':
                batch_df_aligned[feature] = 50
            elif feature == 'BMI':
                batch_df_aligned[feature] = 24.0
            elif feature in CATEGORICAL_FEATURES:
                batch_df_aligned[feature] = CATEGORY_MAPPINGS[feature]['default']
            else:
                batch_df_aligned[feature] = 0.0

    # 4. æ ‡å‡†åŒ–åˆ†ç±»ç‰¹å¾
    for cat_feat in CATEGORICAL_FEATURES:
        if cat_feat in batch_df_aligned.columns:
            batch_df_aligned[cat_feat] = standardize_categorical_feature(
                batch_df_aligned[cat_feat], cat_feat
            )

    # 5. åˆ†ç¦»æ•°å€¼å‹/åˆ†ç±»å‹ç‰¹å¾
    numeric_data = batch_df_aligned[NUMERIC_FEATURES].copy()
    categorical_data = batch_df_aligned[CATEGORICAL_FEATURES].copy()

    # 6. ç¡®ä¿æ•°å€¼æ•°æ®æ˜¯æ•°å€¼ç±»å‹
    for col in numeric_data.columns:
        numeric_data[col] = pd.to_numeric(numeric_data[col], errors='coerce')

    # 7. ç¡®ä¿åˆ†ç±»æ•°æ®æ˜¯æ•´æ•°ç±»å‹
    for col in categorical_data.columns:
        categorical_data[col] = pd.to_numeric(categorical_data[col], errors='coerce').fillna(
            CATEGORY_MAPPINGS[col]['default']
        ).astype(int)

    # 8. å¡«å……ç¼ºå¤±å€¼
    median_imputer = feature_info.get('median_imputer')
    mode_imputer = feature_info.get('mode_imputer')

    if median_imputer and hasattr(median_imputer, 'feature_names_in_'):
        numeric_filled = median_imputer.transform(numeric_data)
        numeric_data = pd.DataFrame(numeric_filled,
                                    columns=numeric_data.columns,
                                    index=numeric_data.index)
    else:
        # ç®€å•ä¸­ä½æ•°å¡«å……
        for col in numeric_data.columns:
            if numeric_data[col].isnull().any():
                median_val = numeric_data[col].median()
                numeric_data[col] = numeric_data[col].fillna(median_val)

    if mode_imputer and hasattr(mode_imputer, 'feature_names_in_'):
        categorical_filled = mode_imputer.transform(categorical_data)
        categorical_data = pd.DataFrame(categorical_filled,
                                        columns=categorical_data.columns,
                                        index=categorical_data.index)
    else:
        # ç®€å•ä¼—æ•°å¡«å……
        for col in categorical_data.columns:
            if categorical_data[col].isnull().any():
                mode_val = categorical_data[col].mode()[0] if not categorical_data[col].mode().empty else \
                    CATEGORY_MAPPINGS[col]['default']
                categorical_data[col] = categorical_data[col].fillna(mode_val)

    # 9. åˆå¹¶æ•°æ®
    processed_data = pd.concat([numeric_data, categorical_data], axis=1)

    # 10. æ ‡å‡†åŒ–ç‰¹å¾ - ä¿®å¤ç‰ˆæœ¬
    scaler = feature_info.get('scaler')
    if scaler:
        try:
            # æ£€æŸ¥æ ‡å‡†åŒ–å™¨è®­ç»ƒæ—¶çš„ç‰¹å¾
            if hasattr(scaler, 'feature_names_in_'):
                scaler_features = list(scaler.feature_names_in_)
                st.info(f"ğŸ” æ ‡å‡†åŒ–å™¨æœŸæœ›çš„ç‰¹å¾: {scaler_features}")

                # ç¡®ä¿æˆ‘ä»¬æ‹¥æœ‰æ ‡å‡†åŒ–å™¨éœ€è¦çš„æ‰€æœ‰ç‰¹å¾
                missing_features = set(scaler_features) - set(processed_data.columns)
                if missing_features:
                    st.warning(f"âš ï¸ æ ‡å‡†åŒ–å™¨éœ€è¦ä»¥ä¸‹ç‰¹å¾ï¼Œä½†æ•°æ®ä¸­ç¼ºå¤±: {missing_features}")
                    # ä¸ºç¼ºå¤±çš„ç‰¹å¾æ·»åŠ é»˜è®¤å€¼
                    for feat in missing_features:
                        if feat == 'Sex':
                            processed_data[feat] = 2  # é»˜è®¤å¥³æ€§
                        elif feat in NUMERIC_FEATURES:
                            processed_data[feat] = 0.0
                        else:
                            processed_data[feat] = 0.0

                # æŒ‰ç…§æ ‡å‡†åŒ–å™¨è®­ç»ƒæ—¶çš„ç‰¹å¾é¡ºåºæ’åˆ—æ•°æ®
                data_for_scaler = processed_data[scaler_features]
                scaled_data = scaler.transform(data_for_scaler)
                # å°†æ ‡å‡†åŒ–åçš„å€¼æ”¾å›processed_data
                processed_data[scaler_features] = scaled_data
            else:
                # å¦‚æœæ²¡æœ‰ç‰¹å¾åç§°å±æ€§ï¼Œä½¿ç”¨åŸå§‹æ–¹æ³•
                numeric_scaled = scaler.transform(numeric_data)
                processed_data[NUMERIC_FEATURES] = numeric_scaled

        except Exception as e:
            st.warning(f"âš ï¸ æ ‡å‡†åŒ–å¤±è´¥: {e}")
            # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨æ–°çš„æ ‡å‡†åŒ–å™¨
            scaler_new = StandardScaler()
            scaled_data = scaler_new.fit_transform(processed_data)
            processed_data = pd.DataFrame(scaled_data, columns=processed_data.columns, index=processed_data.index)

    # 11. ç¡®ä¿ç‰¹å¾é¡ºåºä¸MODEL_FEATURES_ENå®Œå…¨ä¸€è‡´
    processed_data = processed_data.reindex(columns=MODEL_FEATURES_EN)

    return processed_data, batch_df_aligned


# === å•æ‚£è€…é¢„æµ‹å‡½æ•° ===
def predict_single_patient(input_data_dict, model, feature_info):
    """å•æ‚£è€…é¢„æµ‹"""
    try:
        # 1. å°†è¾“å…¥å­—å…¸è½¬æ¢ä¸ºDataFrame
        input_df = pd.DataFrame([input_data_dict])

        # 2. ç›´æ¥ä½¿ç”¨æ‰¹é‡é¢„å¤„ç†å‡½æ•°
        processed_data, original_df = preprocess_batch_data(input_df, feature_info)

        if processed_data is None or processed_data.empty:
            st.error("âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥")
            return None, None, None

        # 3. æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
        if st.session_state.debug_mode:
            st.write("ğŸ” è°ƒè¯•ä¿¡æ¯:")
            st.write("å¤„ç†åæ•°æ®å½¢çŠ¶:", processed_data.shape)
            st.write("å¤„ç†åæ•°æ®å‰3è¡Œ:", processed_data.head(3))

        # 4. é¢„æµ‹
        probability = model.predict_proba(processed_data)[0][1]
        prediction = model.predict(processed_data)[0]

        return probability, prediction, original_df

    except Exception as e:
        st.error(f"é¢„æµ‹å‡ºé”™: {e}")
        import traceback
        st.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return None, None, None


# === æ‰¹é‡é¢„æµ‹å‡½æ•° ===
def batch_predict(batch_df, model, feature_info):
    """æ‰¹é‡é¢„æµ‹"""
    # é¢„å¤„ç†
    processed_data, original_df = preprocess_batch_data(batch_df, feature_info)

    if processed_data is None or processed_data.empty:
        st.error("âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥")
        return None

    # æ˜¾ç¤ºå¤„ç†åçš„æ•°æ®ä¿¡æ¯
    st.info(f"ğŸ“Š å¤„ç†åæ•°æ®å½¢çŠ¶: {processed_data.shape}")

    # é¢„æµ‹
    try:
        probabilities = model.predict_proba(processed_data)[:, 1]
        predictions = model.predict(processed_data)

        # é£é™©ç­‰çº§
        def get_risk_level(prob):
            if prob < 0.9:
                return "ä½é£é™©"
            elif prob < 0.99:
                return "ä¸­é£é™©"
            else:
                return "é«˜é£é™©"

        risk_levels = [get_risk_level(prob) for prob in probabilities]

        # ç»“æœæ•´åˆ
        results_df = original_df.copy()
        results_df['PDRé¢„æµ‹æ¦‚ç‡'] = probabilities
        results_df['PDRé¢„æµ‹ç±»åˆ«'] = predictions
        results_df['PDRé£é™©ç­‰çº§'] = risk_levels
        results_df['é¢„æµ‹æ—¶é—´'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        return results_df

    except Exception as e:
        st.error(f"é¢„æµ‹å‡ºé”™: {e}")
        import traceback
        st.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return None


# === é£é™©ç­‰çº§åˆ’åˆ†å‡½æ•° ===
def get_risk_display_info(probability):
    """æ ¹æ®æ¦‚ç‡è·å–é£é™©æ˜¾ç¤ºä¿¡æ¯"""
    if probability < 0.9:
        return "ä½é£é™©", "âœ…", "#00D4AA"
    elif probability < 0.99:
        return "ä¸­é£é™©", "âš ï¸", "#FFA500"
    else:
        return "é«˜é£é™©", "ğŸ”´", "#FF4B4B"


# === ä¸»å‡½æ•° ===
def main():
    st.title("ğŸ‘ï¸ ç³–å°¿ç—…è§†ç½‘è†œç—…å˜ï¼ˆPDRï¼‰é£é™©é¢„æµ‹ç³»ç»Ÿ")
    st.markdown("---")

    # åŠ è½½æ¨¡å‹
    with st.spinner("åŠ è½½æ¨¡å‹ä¸­..."):
        model, scaler, feature_info, selected_features = load_model_and_preprocessors()

    if model is None:
        st.error("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥final_resultsç›®å½•ä¸‹çš„æ¨¡å‹æ–‡ä»¶ï¼")
        return

    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    if selected_features:
        st.sidebar.header("ğŸ“Š æ¨¡å‹ç‰¹å¾ä¿¡æ¯")
        st.sidebar.info(f"æ¨¡å‹ä½¿ç”¨ {len(selected_features)} ä¸ªç‰¹å¾")
        with st.sidebar.expander("æŸ¥çœ‹ç‰¹å¾åˆ—è¡¨"):
            for i, feat in enumerate(MODEL_FEATURES_EN, 1):
                chinese_name = REVERSE_FEATURE_MAPPING.get(feat, feat)
                st.sidebar.write(f"{i}. {chinese_name} ({feat})")

    # ä¾§è¾¹æ è®¾ç½®
    st.sidebar.header("ğŸ¥ åŒ»ç–—æœºæ„å±‚æ¬¡")
    facility_level = st.sidebar.radio(
        "é€‰æ‹©å±‚æ¬¡ï¼š",
        ["åˆçº§ï¼ˆä»…åŸºæœ¬ä¿¡æ¯ï¼‰", "é«˜çº§ï¼ˆå…¨éƒ¨æŒ‡æ ‡ï¼‰"],
        index=1
    )
    selected_groups = ['basic'] if facility_level == "åˆçº§ï¼ˆä»…åŸºæœ¬ä¿¡æ¯ï¼‰" else ['basic', 'advanced']

    st.sidebar.header("ğŸ” é¢„æµ‹æ¨¡å¼")
    prediction_mode = st.sidebar.radio(
        "é€‰æ‹©æ¨¡å¼ï¼š",
        ["å•æ‚£è€…é¢„æµ‹", "æ‰¹é‡é¢„æµ‹"],
        index=0
    )

    # æ–‡ä»¶æ ¼å¼è®¾ç½®
    if prediction_mode == "æ‰¹é‡é¢„æµ‹":
        st.sidebar.header("ğŸ“ æ‰¹é‡é¢„æµ‹è®¾ç½®")
        file_format = st.sidebar.radio("æ–‡ä»¶æ ¼å¼ï¼š", ["Excel (.xlsx)", "CSV (.csv)"], index=0)

        # ç”Ÿæˆæ¨¡æ¿
        st.sidebar.markdown("### ğŸ“‹ æ•°æ®æ¨¡æ¿")
        template_data = {}
        for group_key in selected_groups:
            for feat in FEATURE_GROUPS[group_key]['features']:
                if feat == 'æ€§åˆ«':
                    template_data[feat] = [1]  # é»˜è®¤ç”·æ€§
                elif feat == 'å¹´é¾„':
                    template_data[feat] = [50]
                elif feat == 'ç³–å°¿ç—…ç—…ç¨‹':
                    template_data[feat] = [5.0]
                elif feat == 'BMI':
                    template_data[feat] = [24.0]
                elif feat == 'è…°è‡€æ¯”':
                    template_data[feat] = [0.9]
                elif feat == 'é«˜è¡€å‹ç—…ç¨‹':
                    template_data[feat] = [0.0]
                elif feat == 'æ”¶ç¼©å‹':
                    template_data[feat] = [120.0]
                elif feat == 'èˆ’å¼ å‹':
                    template_data[feat] = [80.0]
                elif feat == 'è¡€å°¿ç´ æ°®':
                    template_data[feat] = [5.0]
                elif feat == 'è¡€æ¸…è‚Œé…':
                    template_data[feat] = [70.0]
                elif feat == 'å°¿é…¸':
                    template_data[feat] = [300.0]
                elif feat == 'æ€»è›‹ç™½':
                    template_data[feat] = [70.0]
                elif feat == 'ç™½è›‹ç™½':
                    template_data[feat] = [45.0]
                elif feat == 'æ€»èƒ†çº¢ç´ ':
                    template_data[feat] = [12.0]
                elif feat == 'ç›´æ¥èƒ†çº¢ç´ ':
                    template_data[feat] = [4.0]
                elif feat == 'è°·ä¸™è½¬æ°¨é…¶':
                    template_data[feat] = [25.0]
                elif feat == 'è°·è‰è½¬æ°¨é…¶':
                    template_data[feat] = [26.0]
                elif feat == 'ç©ºè…¹è¡€ç³–':
                    template_data[feat] = [6.5]
                elif feat == 'ç³–åŒ–è¡€çº¢è›‹ç™½':
                    template_data[feat] = [6.5]
                elif feat == 'å°¿ç™½è›‹ç™½æ’æ³„ç‡':
                    template_data[feat] = [20.0]
                else:
                    template_data[feat] = [0.0]

        template_df = pd.DataFrame(template_data)

        # æ¨¡æ¿ä¸‹è½½é“¾æ¥
        if file_format == "Excel (.xlsx)":
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                template_df.to_excel(writer, index=False, sheet_name='æ¨¡æ¿')
            excel_data = output.getvalue()
            b64 = base64.b64encode(excel_data).decode()
            href = f'<a href="data:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;base64,{b64}" download="pdré¢„æµ‹æ¨¡æ¿.xlsx">ğŸ“¥ ä¸‹è½½Excelæ¨¡æ¿</a>'
        else:
            csv = template_df.to_csv(index=False, encoding='utf-8-sig')
            b64 = base64.b64encode(csv.encode()).decode()
            href = f'<a href="data:file/csv;base64,{b64}" download="pdré¢„æµ‹æ¨¡æ¿.csv">ğŸ“¥ ä¸‹è½½CSVæ¨¡æ¿</a>'
        st.sidebar.markdown(href, unsafe_allow_html=True)

        st.sidebar.markdown("### ğŸ“ ä½¿ç”¨è¯´æ˜")
        st.sidebar.info("""
        **é‡è¦æç¤ºï¼š**
        1. ä¸‹è½½æ¨¡æ¿å¹¶å¡«å†™æ‚£è€…æ•°æ®
        2. **ç¡®ä¿åˆ†ç±»åˆ—å¡«å†™æ­£ç¡®ï¼š**
           - æ€§åˆ«: 1(ç”·) æˆ– 2(å¥³)
        3. **æ•°å€¼åˆ—åªå¡«å†™æ•°å­—**
        4. ä¸Šä¼ å¡«å†™å¥½çš„æ–‡ä»¶
        5. ç‚¹å‡»"å¼€å§‹æ‰¹é‡é¢„æµ‹"

        **ç‰¹å¾é¡ºåºï¼š**
        ç³»ç»Ÿå°†æŒ‰ç…§ä»¥ä¸‹é¡ºåºå¤„ç†ç‰¹å¾ï¼š
        1. æ€§åˆ« 2. å¹´é¾„ 3. ç³–å°¿ç—…ç—…ç¨‹ 4. BMI 5. è…°è‡€æ¯” 
        6. æ”¶ç¼©å‹ 7. èˆ’å¼ å‹ 8. è¡€å°¿ç´ æ°® 9. è¡€æ¸…è‚Œé… 
        10. å°¿é…¸ 11. æ€»è›‹ç™½ 12. ç™½è›‹ç™½ 13. æ€»èƒ†çº¢ç´  
        14. ç›´æ¥èƒ†çº¢ç´  15. è°·ä¸™è½¬æ°¨é…¶ 16. è°·è‰è½¬æ°¨é…¶ 
        17. ç©ºè…¹è¡€ç³– 18. ç³–åŒ–è¡€çº¢è›‹ç™½ 19. å°¿ç™½è›‹ç™½æ’æ³„ç‡ 
        20. é«˜è¡€å‹ç—…ç¨‹
        """)

    # æ·»åŠ è°ƒè¯•å¼€å…³
    st.session_state.debug_mode = st.sidebar.checkbox("ğŸ” å¯ç”¨è¯¦ç»†è°ƒè¯•æ¨¡å¼", value=False)

    st.sidebar.header("â„¹ï¸ å…³äº")
    st.sidebar.info("æœ¬ç³»ç»ŸåŸºäºLightGBMæ¨¡å‹ï¼Œä»…ä¾›åŒ»ç–—å‚è€ƒï¼")

    # å•æ‚£è€…é¢„æµ‹ç•Œé¢
    if prediction_mode == "å•æ‚£è€…é¢„æµ‹":
        col1, col2 = st.columns([2, 1])

        with col1:
            st.header("ğŸ“ æ‚£è€…ä¿¡æ¯è¾“å…¥")
            st.markdown(f"**å½“å‰å±‚æ¬¡ï¼š{facility_level}**")

            with st.form("prediction_form"):
                input_values = {}
                # æ˜¾ç¤ºè¾“å…¥å­—æ®µ
                for group_key in selected_groups:
                    group = FEATURE_GROUPS[group_key]
                    st.subheader(group['name'])
                    features = group['features']

                    # æ¯è¡Œæ˜¾ç¤º3ä¸ªç‰¹å¾
                    num_rows = (len(features) + 2) // 3
                    for row in range(num_rows):
                        row_feats = features[row * 3:(row + 1) * 3]
                        if row_feats:
                            cols = st.columns(3)
                            for idx, feat in enumerate(row_feats):
                                with cols[idx]:
                                    input_values[feat] = create_input_field_chinese(feat)

                # æäº¤æŒ‰é’®
                submitted = st.form_submit_button("ğŸ” å¼€å§‹é¢„æµ‹", use_container_width=True)

        with col2:
            st.header("ğŸ“Š é¢„æµ‹ç»“æœ")
            if submitted:
                with st.spinner("åˆ†ææ•°æ®ä¸­..."):
                    prob, pred, original_df = predict_single_patient(input_values, model, feature_info)

                    if prob is not None:
                        # è·å–é£é™©ä¿¡æ¯
                        risk_level, risk_icon, risk_color = get_risk_display_info(prob)

                        # æ˜¾ç¤ºåŸå§‹è¾“å…¥å€¼ï¼ˆéªŒè¯ï¼‰
                        if st.session_state.debug_mode:
                            st.subheader("ğŸ“‹ è¾“å…¥éªŒè¯")
                            st.write("æ‚¨è¾“å…¥çš„å€¼:")
                            for i, (feat, val) in enumerate(input_values.items()):
                                st.write(f"{i + 1}. {feat}: {val}")


                        # ç»“æœå±•ç¤º
                        st.metric(f"{risk_icon} é¢„æµ‹ç»“æœ", risk_level, delta=f"{prob * 100:.2f}%")


                        # é£é™©è§£é‡Š
                        if risk_level == "é«˜é£é™©":
                            st.error("""
                            âš ï¸ **é«˜é£é™©é¢„è­¦**:
                            â€¢ ç«‹å³è¿›è¡Œçœ¼ç§‘è¯¦ç»†æ£€æŸ¥
                            â€¢ ä¸¥æ ¼æ§åˆ¶è¡€ç³–/è¡€å‹
                            â€¢ å®šæœŸçœ¼åº•æ£€æŸ¥
                            â€¢ éµåŒ»å˜±å¹²é¢„
                            """)
                        elif risk_level == "ä¸­é£é™©":
                            st.warning("""
                            âš ï¸ **ä¸­é£é™©æç¤º**:
                            â€¢ å»ºè®®è¿›è¡Œçœ¼ç§‘æ£€æŸ¥
                            â€¢ åŠ å¼ºè¡€ç³–/è¡€å‹æ§åˆ¶
                            â€¢ æ¯åŠå¹´å¤æŸ¥ä¸€æ¬¡
                            â€¢ æ³¨æ„è§†åŠ›å˜åŒ–
                            """)
                        else:
                            st.success("""
                            âœ… **ä½é£é™©æç¤º**:
                            â€¢ ä¿æŒè¡€ç³–æ§åˆ¶
                            â€¢ æ¯å¹´ä¸€æ¬¡çœ¼ç§‘æ£€æŸ¥
                            â€¢ å¥åº·ç”Ÿæ´»æ–¹å¼
                            â€¢ è§†åŠ›å˜åŒ–åŠæ—¶å°±åŒ»
                            """)

                        # æä¾›è¯¦ç»†ç»“æœä¸‹è½½
                        if original_df is not None:
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            results_df = original_df.copy()
                            results_df['PDRé¢„æµ‹æ¦‚ç‡'] = prob
                            results_df['PDRé¢„æµ‹ç±»åˆ«'] = pred
                            results_df['PDRé£é™©ç­‰çº§'] = risk_level
                            results_df['é¢„æµ‹æ—¶é—´'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

                            st.download_button(
                                label="ğŸ“¥ ä¸‹è½½è¯¦ç»†æŠ¥å‘Š",
                                data=results_df.to_csv(index=False, encoding='utf-8-sig'),
                                file_name=f"å•æ‚£è€…é¢„æµ‹æŠ¥å‘Š_{timestamp}.csv",
                                mime="text/csv"
                            )

                        st.info("âš ï¸ å…è´£å£°æ˜ï¼šç»“æœä»…ä¾›å‚è€ƒï¼Œä¸æ›¿ä»£ä¸“ä¸šè¯Šæ–­ï¼")
            else:
                st.info("è¯·å¡«å†™å·¦ä¾§ä¿¡æ¯å¹¶ç‚¹å‡»é¢„æµ‹æŒ‰é’®")

    # æ‰¹é‡é¢„æµ‹ç•Œé¢
    else:
        st.header("ğŸ“ æ‰¹é‡é¢„æµ‹")
        st.markdown(f"**å½“å‰çº§åˆ«: {facility_level}**")

        # æ–‡ä»¶ä¸Šä¼ 
        if file_format == "Excel (.xlsx)":
            uploaded_file = st.file_uploader("ä¸Šä¼ Excelæ–‡ä»¶", type=['xlsx', 'xls'],
                                             help="ä½¿ç”¨å·¦ä¾§æ¨¡æ¿ï¼Œæ”¯æŒä¸­æ–‡åˆ—åï¼")
        else:
            uploaded_file = st.file_uploader("ä¸Šä¼ CSVæ–‡ä»¶", type=['csv'],
                                             help="ä½¿ç”¨å·¦ä¾§æ¨¡æ¿ï¼Œæ”¯æŒä¸­æ–‡åˆ—åï¼")

        if uploaded_file is not None:
            try:
                # è¯»å–æ–‡ä»¶
                if uploaded_file.name.endswith(('.xlsx', '.xls')):
                    batch_df = pd.read_excel(uploaded_file, engine='openpyxl')
                else:
                    batch_df = pd.read_csv(uploaded_file)

                st.success(f"âœ… æˆåŠŸè¯»å– {len(batch_df)} è¡Œæ•°æ®ï¼")

                # æ•°æ®é¢„è§ˆ
                st.subheader("ğŸ“‹ æ•°æ®é¢„è§ˆ")
                st.write(f"æ•°æ®å½¢çŠ¶: {batch_df.shape}")
                st.dataframe(batch_df.head(), use_container_width=True)

                # æ‰¹é‡é¢„æµ‹æŒ‰é’®
                if st.button("ğŸš€ å¼€å§‹æ‰¹é‡é¢„æµ‹", type="primary", use_container_width=True):
                    with st.spinner(f"é¢„æµ‹ä¸­ï¼ˆå…±{len(batch_df)}æ¡æ•°æ®ï¼‰..."):
                        results_df = batch_predict(batch_df, model, feature_info)

                    if results_df is not None:
                        # ç»“æœå±•ç¤º
                        st.subheader("ğŸ“Š é¢„æµ‹ç»“æœ")

                        # ç»Ÿè®¡ä¿¡æ¯
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("æ€»æ‚£è€…æ•°", len(results_df))
                        with col2:
                            high_risk = (results_df['PDRé¢„æµ‹æ¦‚ç‡'] > 0.99).sum()
                            st.metric("é«˜é£é™©", high_risk, delta=f"{high_risk / len(results_df) * 100:.1f}%")
                        with col3:
                            mid_risk = ((results_df['PDRé¢„æµ‹æ¦‚ç‡'] >= 0.9) & (results_df['PDRé¢„æµ‹æ¦‚ç‡'] <= 0.99)).sum()
                            st.metric("ä¸­é£é™©", mid_risk, delta=f"{mid_risk / len(results_df) * 100:.1f}%")
                        with col4:
                            low_risk = (results_df['PDRé¢„æµ‹æ¦‚ç‡'] < 0.9).sum()
                            st.metric("ä½é£é™©", low_risk, delta=f"{low_risk / len(results_df) * 100:.1f}%")

                        # é«˜é£é™©æ‚£è€…è¯¦æƒ…
                        high_risk_df = results_df[results_df['PDRé£é™©ç­‰çº§'] == 'é«˜é£é™©']
                        if not high_risk_df.empty:
                            st.warning(f"âš ï¸ å‘ç° {len(high_risk_df)} åé«˜é£é™©æ‚£è€…ï¼")
                            with st.expander("ğŸ”´ é«˜é£é™©æ‚£è€…è¯¦æƒ…"):
                                st.dataframe(high_risk_df, use_container_width=True)

                        # ç»“æœå±•ç¤º
                        st.subheader("ğŸ“‹ è¯¦ç»†ç»“æœ")
                        filter_col1, filter_col2 = st.columns(2)
                        with filter_col1:
                            risk_filter = st.selectbox("æŒ‰é£é™©ç­›é€‰:", ["å…¨éƒ¨", "é«˜é£é™©", "ä¸­é£é™©", "ä½é£é™©"])
                        with filter_col2:
                            prob_filter = st.slider("æŒ‰æ¦‚ç‡ç­›é€‰:", 0.0, 1.0, (0.0, 1.0), 0.01)

                        # åº”ç”¨ç­›é€‰
                        filtered_df = results_df.copy()
                        if risk_filter != "å…¨éƒ¨":
                            filtered_df = filtered_df[filtered_df['PDRé£é™©ç­‰çº§'] == risk_filter]
                        filtered_df = filtered_df[
                            (filtered_df['PDRé¢„æµ‹æ¦‚ç‡'] >= prob_filter[0]) &
                            (filtered_df['PDRé¢„æµ‹æ¦‚ç‡'] <= prob_filter[1])
                            ]
                        st.write(f"ç­›é€‰ç»“æœ: {len(filtered_df)} æ¡")
                        st.dataframe(filtered_df, use_container_width=True)

                        # ä¸‹è½½ç»“æœ
                        st.markdown("### ğŸ’¾ ä¸‹è½½ç»“æœ")
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        col1, col2 = st.columns(2)
                        with col1:
                            excel_name = f"pdré¢„æµ‹ç»“æœ_{timestamp}.xlsx"
                            st.markdown(get_table_download_link(results_df, excel_name, "excel"),
                                        unsafe_allow_html=True)
                        with col2:
                            csv_name = f"pdré¢„æµ‹ç»“æœ_{timestamp}.csv"
                            st.markdown(get_table_download_link(results_df, csv_name, "csv"),
                                        unsafe_allow_html=True)

            except Exception as e:
                st.error(f"å¤„ç†æ–‡ä»¶å‡ºé”™: {str(e)}")
                st.info("""
                **â— å¸¸è§é—®é¢˜è§£å†³ï¼š**
                1. **æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦æœ‰éæ•°å­—å­—ç¬¦**
                2. **ç¡®ä¿åˆ†ç±»åˆ—å¡«å†™æ­£ç¡®ï¼š**
                   - æ€§åˆ«: 1(ç”·) æˆ– 2(å¥³)
                3. **æ•°å€¼åˆ—åªå¡«å†™æ•°å­—**
                4. ä¸‹è½½å·¦ä¾§æ¨¡æ¿ï¼ŒæŒ‰æ¨¡æ¿æ ¼å¼å¡«å†™æ•°æ®
                """)
        else:
            # æ‰¹é‡é¢„æµ‹è¯´æ˜
            st.info(f"""
            ## ğŸ“ æ‰¹é‡é¢„æµ‹è¯´æ˜
            1. ä¸‹è½½å·¦ä¾§æ¨¡æ¿å¹¶å¡«å†™æ•°æ®
            2. ä¸Šä¼ å¡«å†™å¥½çš„æ–‡ä»¶
            3. ç‚¹å‡»"å¼€å§‹æ‰¹é‡é¢„æµ‹"
            4. æŸ¥çœ‹ç»Ÿè®¡ç»“æœå¹¶ä¸‹è½½

            ### ğŸ“‹ æ•°æ®è¦æ±‚
            - åˆ—åä¸æ¨¡æ¿ä¸€è‡´ï¼ˆä¸­æ–‡ï¼‰
            - **åˆ†ç±»åˆ—å¡«ï¼š**
              - æ€§åˆ«: 1(ç”·) æˆ– 2(å¥³)
            - **æ•°å€¼ç‰¹å¾å¡«æ•°å­—**
            - ç³»ç»Ÿä¼šè‡ªåŠ¨æ¸…æ´—æ•°æ®å¹¶å¤„ç†ç¼ºå¤±å€¼

            ### ğŸ”¢ ç‰¹å¾é¡ºåº
            ç³»ç»Ÿå°†æŒ‰ç…§ä»¥ä¸‹é¡ºåºå¤„ç†ç‰¹å¾ï¼š
            1. æ€§åˆ« 2. å¹´é¾„ 3. ç³–å°¿ç—…ç—…ç¨‹ 4. BMI 5. è…°è‡€æ¯” 
            6. æ”¶ç¼©å‹ 7. èˆ’å¼ å‹ 8. è¡€å°¿ç´ æ°® 9. è¡€æ¸…è‚Œé… 
            10. å°¿é…¸ 11. æ€»è›‹ç™½ 12. ç™½è›‹ç™½ 13. æ€»èƒ†çº¢ç´  
            14. ç›´æ¥èƒ†çº¢ç´  15. è°·ä¸™è½¬æ°¨é…¶ 16. è°·è‰è½¬æ°¨é…¶ 
            17. ç©ºè…¹è¡€ç³– 18. ç³–åŒ–è¡€çº¢è›‹ç™½ 19. å°¿ç™½è›‹ç™½æ’æ³„ç‡ 
            20. é«˜è¡€å‹ç—…ç¨‹
            """)

    # åº•éƒ¨ä¿¡æ¯
    st.markdown("---")
    st.markdown(
        "<div style='text-align: center; color: gray;'>åŸºäºLightGBMæ¨¡å‹ | ä»…ä¾›åŒ»ç–—ä¸“ä¸šäººå‘˜å‚è€ƒ</div>",
        unsafe_allow_html=True
    )


if __name__ == "__main__":
    main()